# ü§ñ LinkedIn Jobs Research AI Agent

> A production-ready multi-agent pipeline that scrapes, parses, analyzes, and reports on LinkedIn job postings ‚Äî powered by the **OpenAI Agents SDK**.

[![Python](https://img.shields.io/badge/Python-3.11%2B-blue?logo=python)](https://www.python.org/)
[![OpenAI Agents SDK](https://img.shields.io/badge/OpenAI%20Agents%20SDK-0.0.9%2B-green?logo=openai)](https://github.com/openai/openai-agents-python)
[![uv](https://img.shields.io/badge/package%20manager-uv-purple)](https://github.com/astral-sh/uv)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## üìã Table of Contents

- [What It Does](#-what-it-does)
- [Sample Output](#-sample-output)
- [Pipeline Architecture](#-pipeline-architecture)
- [Project Structure](#-project-structure)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [API Keys Setup](#-api-keys-setup)
- [Configuration](#-configuration)
- [Running the Agent](#-running-the-agent)
- [Output Files](#-output-files)
- [Report Sections](#-report-sections)
- [Data Model](#-data-model)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## ‚ú® What It Does

This agent automatically:

1. **Scrapes** live job postings from LinkedIn (via Playwright browser automation or SerpAPI)
2. **Parses** raw HTML/JSON into structured data ‚Äî titles, companies, skills, locations, experience levels
3. **Analyzes** the data ‚Äî role demand, top skills, work-type trends, hiring companies, regional splits
4. **Generates** a full Markdown report + 6 charts + CSV/JSON exports

**Use cases:**
- Understand which tech roles are in highest demand
- See which skills are most requested across thousands of real job listings
- Compare US vs India job markets
- Track hiring trends over the last 24 months
- Research before a job search or career pivot

---

## üìÇ Sample Output

Don't want to run the agent just yet? Browse the pre-generated results in the [`sample_output/`](./sample_output/) folder:

| File | Preview |
|---|---|
| üìÑ [`linkedin_jobs_report.md`](./sample_output/linkedin_jobs_report.md) | Full research report (~6,329 real jobs, US + India, with AI/ML adoption analysis) |
| üìä [`jobs_data.csv`](./sample_output/jobs_data.csv) | Open in Excel / Google Sheets |
| üóÇ [`jobs_data.json`](./sample_output/jobs_data.json) | Structured JSON for all job postings |

**Sample Charts:**

| Role Demand | Top Skills |
|---|---|
| ![Role Demand](./sample_output/charts/role_demand.png) | ![Top Skills](./sample_output/charts/top_skills.png) |

| Experience Distribution | Work Type |
|---|---|
| ![Experience](./sample_output/charts/experience_distribution.png) | ![Work Type](./sample_output/charts/work_type_distribution.png) |

| Quarterly Trends | Region Split |
|---|---|
| ![Trends](./sample_output/charts/quarterly_trends.png) | ![Region](./sample_output/charts/region_split.png) |

| AI/ML Knowledge Expected by Role | |
|---|---|
| ![AI Mention](./sample_output/charts/ai_mention_by_role.png) | *New: which roles are expecting AI/ML skills ‚Äî even outside core AI/ML titles* |

> üìå This sample was generated on 2026-02-26 using SerpAPI mode with **6,329 real job postings**. Clone the repo and run the agent to get a fresh report with today's live data.

---

## üèó Pipeline Architecture

```
main.py (orchestrator)
  ‚îÇ
  ‚îú‚îÄ‚îÄ 1. Scraper Agent    ‚Üê Collects raw job pages
  ‚îÇ       ‚îî‚îÄ‚îÄ pipeline/scraper_agent.py
  ‚îÇ           ‚îú‚îÄ‚îÄ Mode A: Playwright (browser automation, stealth)
  ‚îÇ           ‚îî‚îÄ‚îÄ Mode B: SerpAPI (Google Jobs API ‚Äî recommended)
  ‚îÇ
  ‚îú‚îÄ‚îÄ 2. Parser Agent     ‚Üê Extracts structured data from raw pages
  ‚îÇ       ‚îî‚îÄ‚îÄ pipeline/parser_agent.py + tools/parser_tool.py
  ‚îÇ
  ‚îú‚îÄ‚îÄ 3. Analyst Agent    ‚Üê Computes stats + generates LLM insights
  ‚îÇ       ‚îî‚îÄ‚îÄ pipeline/analyst_agent.py
  ‚îÇ
  ‚îî‚îÄ‚îÄ 4. Report Agent     ‚Üê Writes report, charts, CSV, JSON
          ‚îî‚îÄ‚îÄ pipeline/report_agent.py + tools/chart_tool.py
```

**Framework:** [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) &nbsp;|&nbsp; **Package Manager:** [uv](https://github.com/astral-sh/uv)

---

## üìÅ Project Structure

```
linkedin_jobs_agent/
‚îú‚îÄ‚îÄ main.py                    # Entry point & CLI
‚îú‚îÄ‚îÄ config.yaml                # All settings (roles, locations, API keys, etc.)
‚îú‚îÄ‚îÄ pyproject.toml             # Dependencies (managed by uv)
‚îÇ
‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îú‚îÄ‚îÄ scraper_agent.py       # Playwright + SerpAPI scraper
‚îÇ   ‚îú‚îÄ‚îÄ parser_agent.py        # HTML/JSON ‚Üí JobPosting objects
‚îÇ   ‚îú‚îÄ‚îÄ analyst_agent.py       # Stats aggregation + LLM insights
‚îÇ   ‚îî‚îÄ‚îÄ report_agent.py        # Report builder + chart exporter
‚îÇ
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ browser_tool.py        # Playwright stealth browser wrapper
‚îÇ   ‚îú‚îÄ‚îÄ parser_tool.py         # BeautifulSoup + SerpAPI JSON parsers
‚îÇ   ‚îî‚îÄ‚îÄ chart_tool.py          # Matplotlib chart generators (6 chart types)
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ job_schema.py          # Pydantic v2 models for all data structures
‚îÇ
‚îî‚îÄ‚îÄ output/                    # Auto-generated (git-ignored)
    ‚îú‚îÄ‚îÄ linkedin_jobs_report.md
    ‚îú‚îÄ‚îÄ jobs_data.json
    ‚îú‚îÄ‚îÄ jobs_data.csv
    ‚îî‚îÄ‚îÄ charts/
        ‚îú‚îÄ‚îÄ role_demand.png
        ‚îú‚îÄ‚îÄ top_skills.png
        ‚îú‚îÄ‚îÄ experience_distribution.png
        ‚îú‚îÄ‚îÄ work_type_distribution.png
        ‚îú‚îÄ‚îÄ quarterly_trends.png
        ‚îî‚îÄ‚îÄ region_split.png
```

---

## ‚úÖ Prerequisites

Before you start, make sure you have:

| Requirement | Version | Notes |
|---|---|---|
| **Python** | 3.11 or higher | [Download](https://www.python.org/downloads/) |
| **uv** | latest | Fast Python package manager (replaces pip + venv) |
| **OpenAI API Key** | ‚Äî | Required for analysis & insights |
| **SerpAPI Key** | ‚Äî | Required only for `--mode serpapi` (recommended) |

> **Don't have uv?** Install it in one command ‚Äî see [Installation](#-installation) step 1.

---

## üöÄ Installation

### Step 1 ‚Äî Install `uv` (package manager)

**macOS / Linux:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Windows (PowerShell):**
```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

After installing, restart your terminal, then verify:
```bash
uv --version
```

---

### Step 2 ‚Äî Clone the repository

```bash
git clone https://github.com/shashipk/LinkedIn_Jobs_Research_AI_Agent.git
cd LinkedIn_Jobs_Research_AI_Agent
```

---

### Step 3 ‚Äî Install Python dependencies

```bash
uv sync
```

This creates a virtual environment (`.venv/`) and installs all packages from `pyproject.toml` automatically. No need to run `pip install` manually.

---

### Step 4 ‚Äî Install Playwright browsers

```bash
uv run playwright install chromium
```

> This downloads the Chromium browser used for LinkedIn scraping. Only needed for Playwright mode.

---

## üîë API Keys Setup

### OpenAI API Key (Required)

The agent uses OpenAI for LLM-powered insights in the Analyst step.

1. Go to [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
2. Create a new key
3. Set it as an environment variable:

**macOS / Linux:**
```bash
export OPENAI_API_KEY="sk-..."
```

**Windows (Command Prompt):**
```cmd
set OPENAI_API_KEY=sk-...
```

**Windows (PowerShell):**
```powershell
$env:OPENAI_API_KEY = "sk-..."
```

> üí° **Tip:** Add the export line to your `~/.zshrc` or `~/.bashrc` to avoid re-entering it every session.

---

### SerpAPI Key (Required for `serpapi` mode ‚Äî Recommended)

SerpAPI is the most reliable way to collect real job data. It queries Google Jobs and returns clean structured results.

1. Sign up at [serpapi.com](https://serpapi.com/) ‚Äî free plan gives **100 searches/month**
2. Copy your API key from the dashboard
3. Set it via environment variable **or** in `config.yaml`:

**Option A ‚Äî Environment variable (recommended, keeps key out of code):**
```bash
export SERPAPI_API_KEY="your_key_here"
```

**Option B ‚Äî Directly in `config.yaml`:**
```yaml
data_source:
  serpapi:
    api_key: "your_key_here"
```

> ‚ö†Ô∏è **Security:** Never commit your API key to Git. The `.gitignore` already excludes `.env` files. Use environment variables when possible.

---

## ‚öôÔ∏è Configuration

All settings live in `config.yaml`. Open it and customize before running.

### Key Settings

#### Data Source
```yaml
data_source:
  mode: "serpapi"      # "serpapi" (recommended) or "playwright"
  serpapi:
    api_key: "PASTE_YOUR_SERPAPI_KEY_HERE"  # or use SERPAPI_API_KEY env var
    pages_per_query: 1000  # "unlimited" ‚Äî fetches all available pages per query
```

**How many jobs will I get?**

> üí° `pages_per_query: 1000` means **"fetch everything available"** ‚Äî Google Jobs typically caps at 10‚Äì30 pages per query, so it won't actually burn 1,000 credits. It just keeps paginating until results run out.
>
> The real lever for getting more jobs is adding more **roles** and **locations** ‚Äî not `pages_per_query`.

| Roles | Locations | Est. Unique Jobs | Credits Used | Plan Needed |
|---|---|---|---|---|
| 13 | 2 (country-only) | ~500 | ~26 | Free plan ‚úÖ |
| 13 | 12 (cities) | ~2,465 | ~400 | Paid plan |
| **21** | **18 (default)** | **~5,000+** | **~600** | **Paid plan** |
| 21 | 25+ | ~8,000+ | ~900+ | Paid plan |

> Formula: `roles √ó locations √ó actual_pages_available ‚âà total credits`

---

#### Job Roles to Search
21 roles are configured by default, covering the full spectrum of tech jobs:
```yaml
search:
  roles:
    # Software Engineering
    - "Software Engineer"
    - "Backend Engineer"
    - "Frontend Engineer"
    - "Full Stack Engineer"
    - "Staff Engineer"
    - "iOS Engineer"
    - "Android Engineer"
    # AI / Data
    - "Machine Learning Engineer"
    - "AI Engineer"
    - "Data Engineer"
    - "Data Scientist"
    # Infrastructure / Cloud
    - "DevOps Engineer"
    - "Platform Engineer"
    - "Site Reliability Engineer"
    - "Cloud Engineer"
    - "Infrastructure Engineer"
    # Security & Architecture
    - "Security Engineer"
    - "Solutions Architect"
    # Leadership & Quality
    - "Engineering Manager"
    - "Tech Lead"
    - "QA Engineer"
```

---

#### Locations
18 locations are configured by default ‚Äî 9 US cities, 7 India cities, plus country-level fallbacks:
```yaml
search:
  locations:
    # Country-level (broad fallback)
    - name: "United States"
    - name: "India"
    # US Cities
    - name: "San Francisco, California"
    - name: "New York, New York"
    - name: "Seattle, Washington"
    - name: "Austin, Texas"
    - name: "Boston, Massachusetts"
    - name: "Chicago, Illinois"
    - name: "Los Angeles, California"
    - name: "Denver, Colorado"
    - name: "Atlanta, Georgia"
    # India Cities
    - name: "Bengaluru, Karnataka, India"
    - name: "Hyderabad, Telangana, India"
    - name: "Mumbai, Maharashtra, India"
    - name: "Pune, Maharashtra, India"
    - name: "Delhi, India"
    - name: "Chennai, Tamil Nadu, India"
    - name: "Noida, Uttar Pradesh, India"
```

---

#### OpenAI Model
```yaml
openai:
  model: "gpt-4o-mini"          # Used by Parser Agent
  analyst_model: "gpt-4o-mini"  # Used by Analyst Agent
  temperature: 0.2
  max_tokens: 65536
```

---

## ‚ñ∂Ô∏è Running the Agent

### Quick Test (No API keys needed)
```bash
uv run python main.py --demo
```
Runs the full pipeline on **1,000 synthetic jobs** ‚Äî great for testing the setup before spending API credits.

---

### Real Run with SerpAPI (Recommended)
```bash
export OPENAI_API_KEY="sk-..."
export SERPAPI_API_KEY="your_serpapi_key"

uv run python main.py --mode serpapi
```

---

### Real Run with Playwright (LinkedIn direct scraping)
```bash
export OPENAI_API_KEY="sk-..."

uv run python main.py --mode playwright
```

> ‚ö†Ô∏è LinkedIn may block automated browsers. If you see CAPTCHAs, switch to `--mode serpapi`.

---

### All CLI Options

```bash
uv run python main.py [OPTIONS]
```

| Flag | Default | Description |
|---|---|---|
| `--demo` | off | Run with synthetic data (no API keys needed) |
| `--mode` | from config | `serpapi` or `playwright` |
| `--config` | `config.yaml` | Path to a custom config file |
| `--output` | `output/` | Directory to save results |
| `--max-pages` | from config | Override `pages_per_query` at runtime |

**Examples:**
```bash
# Demo mode ‚Äî test the pipeline
uv run python main.py --demo

# SerpAPI with 5 pages per query
uv run python main.py --mode serpapi --max-pages 5

# Custom config file
uv run python main.py --config my_custom_config.yaml

# Custom output directory
uv run python main.py --output ./results/run1
```

---

## üìä Output Files

After a successful run, the `output/` folder contains:

| File | Description |
|---|---|
| `linkedin_jobs_report.md` | Full research report in Markdown |
| `jobs_data.json` | All parsed job postings (structured JSON) |
| `jobs_data.csv` | Flat CSV ‚Äî open in Excel / Google Sheets |
| `charts/role_demand.png` | Bar chart: job count by role + region |
| `charts/top_skills.png` | Top 20 in-demand skills |
| `charts/experience_distribution.png` | Entry / Mid / Senior / Staff split |
| `charts/work_type_distribution.png` | Remote / Hybrid / Onsite split |
| `charts/quarterly_trends.png` | Posting trends over last 24 months |
| `charts/region_split.png` | US vs India vs Other |

### Viewing the Markdown Report

**Option 1 ‚Äî VS Code:**
Open `linkedin_jobs_report.md` ‚Üí press `Cmd+Shift+V` (Mac) or `Ctrl+Shift+V` (Windows)

**Option 2 ‚Äî Convert to HTML and open in browser:**
```bash
uv run python -c "
import markdown, pathlib
md = pathlib.Path('output/linkedin_jobs_report.md').read_text()
html = '<meta charset=utf-8>' + markdown.markdown(md, extensions=['tables','fenced_code'])
pathlib.Path('output/report.html').write_text(html)
print('Saved ‚Üí output/report.html')
"
open output/report.html   # macOS
# xdg-open output/report.html   # Linux
# start output/report.html      # Windows
```

---

## üìÑ Report Sections

The generated report includes:

1. **Executive Summary** ‚Äî total jobs found, date range, data source, top highlights
2. **Total Jobs by Region** ‚Äî US vs India vs Other breakdown
3. **Role Demand Breakdown** ‚Äî table + chart showing which roles are most in demand
4. **Top 20 Skills in Demand** ‚Äî extracted from job titles and descriptions
5. **Experience Level Distribution** ‚Äî Entry / Mid / Senior / Staff / Principal
6. **Work Mode Distribution** ‚Äî Remote / Hybrid / Onsite percentages
7. **Top Hiring Companies** ‚Äî top 20 companies by number of open roles
8. **Quarterly Trends** ‚Äî job posting volume by quarter over 24 months
9. **Observations & Insights** ‚Äî LLM-generated market analysis

---

## üóÇ Data Model

Each scraped job is stored as a `JobPosting` object:

```python
JobPosting(
    job_id: str,
    title: str,                          # Raw job title from listing
    normalized_category: RoleCategory,   # ML/AI, Backend, Frontend, DevOps, etc.
    company_name: str,
    company_size: str | None,
    location_raw: str,
    location_city: str | None,
    location_country: str | None,
    region: Region,                      # US | India | Other
    work_type: WorkType,                 # Remote | Hybrid | Onsite
    date_posted: datetime | None,
    required_skills: list[str],          # Extracted from title + description
    experience_level: ExperienceLevel,   # Entry | Mid | Senior | Staff | Principal
    employment_type: EmploymentType,     # Full-time | Contract | Part-time
    source_url: str,
    search_query: str,
    search_location: str,
)
```

---

## üîß Troubleshooting

### `ModuleNotFoundError: No module named 'agents'`
```bash
uv sync
```
Re-runs dependency installation. All packages will be installed into the `.venv/` folder.

---

### `playwright._impl._errors.Error: Executable doesn't exist`
```bash
uv run playwright install chromium
```

---

### LinkedIn blocks scraping / CAPTCHA detected
- Switch to SerpAPI mode: `uv run python main.py --mode serpapi`
- Or try running with a visible browser: set `headless: false` in `config.yaml`
- Increase delay: raise `min_delay` and `max_delay` in `config.yaml`

---

### SerpAPI returns 0 results
- Make sure your key is valid and not expired
- Check you have remaining credits at [serpapi.com/dashboard](https://serpapi.com/dashboard)
- Make sure `SERPAPI_API_KEY` env var is set (takes priority over config.yaml)
- Try: `uv run python main.py --demo` to verify the rest of the pipeline works

---

### OpenAI API errors
- Verify `OPENAI_API_KEY` is set: `echo $OPENAI_API_KEY`
- Check you have API credits at [platform.openai.com/usage](https://platform.openai.com/usage)
- The pipeline still runs without an API key ‚Äî insights section will be skipped

---

### `uv: command not found`
Re-install uv and restart your terminal:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.zshrc   # or source ~/.bashrc
```

---

### Virtual environment issues
```bash
rm -rf .venv
uv sync
```

---

## ü§ù Contributing

Contributions are welcome!

1. Fork the repo
2. Create a feature branch: `git checkout -b feature/my-feature`
3. Make your changes
4. Run linting: `uv run ruff check .`
5. Format code: `uv run black .`
6. Commit: `git commit -m "feat: describe your change"`
7. Push and open a Pull Request

---

## üìú License

MIT ‚Äî free to use, modify, and distribute.

---

## üôè Acknowledgements

- [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) ‚Äî multi-agent framework
- [SerpAPI](https://serpapi.com/) ‚Äî Google Jobs data
- [Playwright](https://playwright.dev/) ‚Äî browser automation
- [uv](https://github.com/astral-sh/uv) ‚Äî blazing fast Python package manager
- [Rich](https://github.com/Textualize/rich) ‚Äî beautiful terminal output
